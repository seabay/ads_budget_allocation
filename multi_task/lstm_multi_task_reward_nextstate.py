
# ä½¿ç”¨ PyTorch å®ç°çš„å¤šä»»åŠ¡ LSTM æ¨¡æ‹Ÿå™¨ä»£ç æ¡†æ¶ï¼šå®ƒæ¥æ”¶ä¸€æ®µ (state, action) åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸¤ä¸ªç»“æœï¼š
# reward: é¢„æµ‹æœªæ¥ 7 å¤© signupï¼ˆå•å€¼ï¼‰
# next_state: é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€å‘é‡ï¼ˆå‘é‡ï¼‰

import torch
import torch.nn as nn

class MultiTaskLSTMEnvModel(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128, lstm_layers=1):
        super().__init__()
        self.input_dim = state_dim + action_dim
        self.hidden_dim = hidden_dim

        # Shared LSTM
        self.lstm = nn.LSTM(
            input_size=self.input_dim,
            hidden_size=hidden_dim,
            num_layers=lstm_layers,
            batch_first=True
        )

        # Head for reward prediction
        self.reward_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # scalar reward
        )

        # Head for next state prediction (state delta)
        self.state_head = nn.Sequential(
            nn.Linear(hidden_dim, 64),
            nn.ReLU(),
            nn.Linear(64, state_dim)  # vector next_state
        )

    def forward(self, state_seq, action_seq):
        """
        Args:
            state_seq: [batch, seq_len, state_dim]
            action_seq: [batch, seq_len, action_dim]
        Returns:
            reward_pred: [batch, 1]
            next_state_pred: [batch, state_dim]
        """
        x = torch.cat([state_seq, action_seq], dim=-1)  # [batch, seq_len, state+action]
        lstm_out, _ = self.lstm(x)  # [batch, seq_len, hidden_dim]
        last_hidden = lstm_out[:, -1]  # use last timestep output

        reward_pred = self.reward_head(last_hidden)
        next_state_pred = self.state_head(last_hidden)

        return reward_pred, next_state_pred



# Example dimensions
state_dim = 45
action_dim = 45
seq_len = 5
batch_size = 32

# Fake input
state_seq = torch.randn(batch_size, seq_len, state_dim)
action_seq = torch.randn(batch_size, seq_len, action_dim)

# Model
model = MultiTaskLSTMEnvModel(state_dim, action_dim)
reward_pred, next_state_pred = model(state_seq, action_seq)

print("reward_pred shape:", reward_pred.shape)         # [batch_size, 1]
print("next_state_pred shape:", next_state_pred.shape) # [batch_size, state_dim]


###########################

# MultiTaskLSTMEnvModel çš„ è®­ç»ƒä»£ç æ¡†æ¶ï¼Œç”¨äºè®­ç»ƒè¯¥æ¨¡å‹é¢„æµ‹ reward å’Œ next_stateã€‚
# æˆ‘ä»¬å‡è®¾ä½ å·²æœ‰æ—¥å¿—æ•°æ® (state_seq, action_seq, reward, next_state)

def multitask_loss(reward_true, reward_pred, state_next_true, state_next_pred, Î»=1.0):
    loss_r = nn.MSELoss()(reward_pred, reward_true)
    loss_s = nn.MSELoss()(state_next_pred, state_next_true)
    return loss_r + Î» * loss_s


from torch.utils.data import Dataset, DataLoader

class BudgetSequenceDataset(Dataset):
    def __init__(self, state_seqs, action_seqs, rewards, next_states):
        """
        state_seqs: [N, seq_len, state_dim]
        action_seqs: [N, seq_len, action_dim]
        rewards: [N, 1]
        next_states: [N, state_dim]
        """
        self.state_seqs = state_seqs
        self.action_seqs = action_seqs
        self.rewards = rewards
        self.next_states = next_states

    def __len__(self):
        return len(self.rewards)

    def __getitem__(self, idx):
        return (
            self.state_seqs[idx],
            self.action_seqs[idx],
            self.rewards[idx],
            self.next_states[idx]
        )


def train_lstm_env_model(
    model,
    dataloader,
    num_epochs=20,
    lr=1e-3,
    lambda_weight=1.0,
    device="cuda" if torch.cuda.is_available() else "cpu"
):
    model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    loss_fn = nn.MSELoss()

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0

        for state_seq, action_seq, reward, next_state in dataloader:
            state_seq = state_seq.to(device).float()
            action_seq = action_seq.to(device).float()
            reward = reward.to(device).float()
            next_state = next_state.to(device).float()

            pred_reward, pred_next_state = model(state_seq, action_seq)

            loss_r = loss_fn(pred_reward, reward)
            loss_s = loss_fn(pred_next_state, next_state)
            loss = loss_r + lambda_weight * loss_s

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"[Epoch {epoch+1}] Loss: {avg_loss:.4f}")


# ä½¿ç”¨ç¤ºä¾‹ï¼ˆå¸¦æ¨¡æ‹Ÿæ•°æ®ï¼‰
# Hyperparams
state_dim = 45
action_dim = 45
seq_len = 5
n_samples = 1000
batch_size = 32

# Fake dataset
state_seqs = torch.randn(n_samples, seq_len, state_dim)
action_seqs = torch.randn(n_samples, seq_len, action_dim)
rewards = torch.randn(n_samples, 1)
next_states = torch.randn(n_samples, state_dim)

# Dataloader
dataset = BudgetSequenceDataset(state_seqs, action_seqs, rewards, next_states)
loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Train
model = MultiTaskLSTMEnvModel(state_dim, action_dim)
train_lstm_env_model(model, loader, num_epochs=10)


###########################

# å¦‚ä½•ç”¨ è®­ç»ƒå¥½çš„ LSTM æ¨¡æ‹Ÿå™¨ æ¥åš offline RL ä¸­çš„ rollout â€”â€” å³ï¼šä»ä¸€ä¸ªå½“å‰çŠ¶æ€ s_t å’Œç­–ç•¥ç”Ÿæˆçš„åŠ¨ä½œ a_t å‡ºå‘ï¼Œ
# æ¨¡æ‹Ÿå¾—åˆ° (s_t, a_t, r_t, s_{t+1}) å››å…ƒç»„ã€‚è¿™ä¸ªè¿‡ç¨‹å¸¸ç”¨äºï¼š
# * è®­ç»ƒ Q ç½‘ç»œ
# * è’™ç‰¹å¡æ´›è¯„ä¼°ä¸€ä¸ªç­–ç•¥
# * ç”Ÿæˆé¢å¤–åˆæˆæ•°æ®ï¼ˆmodel-based offline RLï¼‰



def rollout_simulated_transition(
    model,         # trained LSTM environment model
    policy,        # a policy: takes state_seq and returns action
    state_seq,     # [seq_len, state_dim]
    action_seq,    # [seq_len, action_dim]
    device="cpu"
):
    """
    Perform a 1-step rollout from LSTM-based environment.
    Returns: (s_t, a_t, r_t, s_{t+1})
    """
    model.eval()
    with torch.no_grad():
        # Convert to batch size = 1
        state_seq_tensor = torch.tensor(state_seq).unsqueeze(0).float().to(device)  # [1, seq_len, state_dim]
        action_seq_tensor = torch.tensor(action_seq).unsqueeze(0).float().to(device)  # [1, seq_len, action_dim]

        # Generate new action from current state (e.g. latest state in sequence)
        current_state = state_seq[-1]  # [state_dim]
        a_t = policy(current_state)  # action at t (should be [action_dim])

        # Append a_t to action_seq, and update state_seq (e.g. assume current state is repeated)
        new_state_seq = torch.cat([state_seq_tensor[:, 1:], state_seq_tensor[:, -1:].clone()], dim=1)
        new_action_seq = torch.cat([action_seq_tensor[:, 1:], torch.tensor(a_t).view(1, 1, -1).to(device)], dim=1)

        # Predict reward and next state using simulator
        r_pred, s_next_pred = model(new_state_seq, new_action_seq)

        return current_state, a_t, r_pred.squeeze(0).cpu().numpy(), s_next_pred.squeeze(0).cpu().numpy()


# å‡è®¾å½“å‰å†å²çª—å£é•¿åº¦ä¸º 5
seq_len = 5
state_dim = 45
action_dim = 45

# å‡å†å²è½¨è¿¹
state_seq = torch.randn(seq_len, state_dim)
action_seq = torch.randn(seq_len, action_dim)

# å‡ç­–ç•¥ï¼ˆéšæœºï¼‰
def random_policy(state):
    return torch.randn(action_dim).numpy()

# ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ rollout
s_t, a_t, r_t, s_tp1 = rollout_simulated_transition(
    model=model,  # è®­ç»ƒå¥½çš„ MultiTaskLSTMEnvModel
    policy=random_policy,
    state_seq=state_seq,
    action_seq=action_seq
)

print("s_t shape:", s_t.shape)
print("a_t shape:", a_t.shape)
print("r_t:", r_t)
print("s_{t+1} shape:", s_tp1.shape)


###########################

# Rollout æ•°æ®æ¥å…¥ CQL æ¨¡å‹çš„è®­ç»ƒæµç¨‹

import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader

# æ•°æ®é›†æ„å»ºï¼šå°† rollout (s, a, r, s') è½¬ä¸ºå¯è®­ç»ƒæ ¼å¼
# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®é›† (å¯ä»¥æ›¿æ¢ä¸º rollout çš„çœŸå®æ•°æ®)
def generate_synthetic_dataset(n_samples=1000, state_dim=45, action_dim=45):
    dataset = []
    for _ in range(n_samples):
        s = np.random.randn(state_dim).astype(np.float32)
        a = np.random.randn(action_dim).astype(np.float32)
        r = np.random.rand(1).astype(np.float32)
        s_next = s + np.random.normal(0, 0.1, size=state_dim).astype(np.float32)
        dataset.append((s, a, r, s_next))
    return dataset

# è‡ªå®šä¹‰ PyTorch Dataset
class TransitionDataset(Dataset):
    def __init__(self, data):
        self.states = torch.tensor([d[0] for d in data])
        self.actions = torch.tensor([d[1] for d in data])
        self.rewards = torch.tensor([d[2] for d in data])
        self.next_states = torch.tensor([d[3] for d in data])

    def __len__(self):
        return len(self.states)

    def __getitem__(self, idx):
        return (
            self.states[idx],
            self.actions[idx],
            self.rewards[idx],
            self.next_states[idx],
        )

# æ„å»ºæ•°æ®
synthetic_data = generate_synthetic_dataset()
dataset = TransitionDataset(synthetic_data)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

# ğŸ§  2. æ¥å…¥ CQL æˆ–ä»»ä½• Offline RL ç®—æ³•
for epoch in range(num_epochs):
    for s, a, r, s_next in dataloader:
        # è¾“å…¥åˆ° Offline RL ç®—æ³•
        # cql.train_step(s, a, r, s_next) æˆ–è€…å…¶ä»–ç®—æ³•
        pass


###########################

